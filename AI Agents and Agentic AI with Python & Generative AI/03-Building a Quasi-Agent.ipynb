{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23653c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "\n",
    "# %pip install litellm \n",
    "from litellm import completion # serves as the bridge between your code and the LLM, allowing you to send prompts and receive responses in a structured and efficient way\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "935f8a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# Initialize and constants\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337b45e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor this exercise, you should write a program that uses sequential prompts to generate any Python function based on user input. The program should:\\n\\nFirst Prompt:\\n-Ask the user what function they want to create\\n-Ask the LLM to write a basic Python function based on the user’s description\\n-Store the response for use in subsequent prompts\\n-Parse the response to separate the code from the commentary by the LLM\\n\\nSecond Prompt:\\n-Pass the code generated from the first prompt\\n-Ask the LLM to add comprehensive documentation including:\\n-Function description\\n-Parameter descriptions\\n-Return value description\\n-Example usage\\n-Edge cases\\n\\nThird Prompt:\\n-Pass the documented code generated from the second prompt\\n-Ask the LLM to add test cases using Python’s unittest framework\\n-Tests should cover:\\n-Basic functionality\\n-Edge cases\\n-Error cases\\n-Various input scenarios\\n\\nRequirements:\\n-Use the LiteLLM library\\n-Maintain conversation context between prompts\\n-Print each step of the development process\\n-Save the final version to a Python file\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "For this exercise, you should write a program that uses sequential prompts to generate any Python function based on user input. The program should:\n",
    "\n",
    "First Prompt:\n",
    "-Ask the user what function they want to create\n",
    "-Ask the LLM to write a basic Python function based on the user’s description\n",
    "-Store the response for use in subsequent prompts\n",
    "-Parse the response to separate the code from the commentary by the LLM\n",
    "\n",
    "Second Prompt:\n",
    "-Pass the code generated from the first prompt\n",
    "-Ask the LLM to add comprehensive documentation including:\n",
    "-Function description\n",
    "-Parameter descriptions\n",
    "-Return value description\n",
    "-Example usage\n",
    "-Edge cases\n",
    "\n",
    "Third Prompt:\n",
    "-Pass the documented code generated from the second prompt\n",
    "-Ask the LLM to add test cases using Python’s unittest framework\n",
    "-Tests should cover:\n",
    "-Basic functionality\n",
    "-Edge cases\n",
    "-Error cases\n",
    "-Various input scenarios\n",
    "\n",
    "Requirements:\n",
    "-Use the LiteLLM library\n",
    "-Maintain conversation context between prompts\n",
    "-Print each step of the development process\n",
    "-Save the final version to a Python file\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e649a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(messages: List[Dict]) -> str:\n",
    "    \"\"\"Call LLM to get response\"\"\"\n",
    "    response = completion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0eb5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function handles our LLM interactions using ChatML format. Each message includes a role (“system”, “user”, or “assistant”) and content.\n",
    "def extract_code_block(response: str) -> str:\n",
    "    \"\"\"Extract code block from response\"\"\"\n",
    "    if not '```' in response:\n",
    "        return response\n",
    "\n",
    "    code_block = response.split('```')[1].strip()\n",
    "    if code_block.startswith(\"python\"):\n",
    "        code_block = code_block[6:]\n",
    "\n",
    "    return code_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0d26d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What kind of function would you like to create?\n",
      "Example: 'A function that calculates the factorial of a number'\n",
      "Your description: "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     67\u001b[39m    \u001b[38;5;28;01mreturn\u001b[39;00m documented_function, test_cases, filename\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m    function_code, tests, filename = \u001b[43mdevelop_custom_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m    \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFinal code has been saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mdevelop_custom_function\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# First prompt - Basic function\u001b[39;00m\n\u001b[32m     14\u001b[39m messages.append({\n\u001b[32m     15\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m    \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWrite a Python function that \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Output the function in a ```python code block```.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m })\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m initial_function = \u001b[43mgenerate_response\u001b[49m(messages)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Parse the response to get the function code\u001b[39;00m\n\u001b[32m     21\u001b[39m initial_function = extract_code_block(initial_function)\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_response' is not defined"
     ]
    }
   ],
   "source": [
    "def develop_custom_function():\n",
    "   # Get user input for function description\n",
    "   print(\"\\nWhat kind of function would you like to create?\")\n",
    "   print(\"Example: 'A function that calculates the factorial of a number'\")\n",
    "   print(\"Your description: \", end='')\n",
    "   function_description = input().strip()\n",
    "\n",
    "   # Initialize conversation with system prompt\n",
    "   messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a Python expert helping to develop a function.\"}\n",
    "   ]\n",
    "\n",
    "   # First prompt - Basic function\n",
    "   messages.append({\n",
    "      \"role\": \"user\",\n",
    "      \"content\": f\"Write a Python function that {function_description}. Output the function in a ```python code block```.\"\n",
    "   })\n",
    "   initial_function = generate_response(messages)\n",
    "\n",
    "   # Parse the response to get the function code\n",
    "   initial_function = extract_code_block(initial_function)\n",
    "\n",
    "   print(\"\\n=== Initial Function ===\")\n",
    "   print(initial_function)\n",
    "\n",
    "   # Add assistant's response to conversation\n",
    "   # Notice that I am purposely causing it to forget its commentary and just see the code so that\n",
    "   # it appears that is always outputting just code.\n",
    "   messages.append({\"role\": \"assistant\", \"content\": \"\\`\\`\\`python\\n\\n\"+initial_function+\"\\n\\n\\`\\`\\`\"})\n",
    "\n",
    "   # Second prompt - Add documentation\n",
    "   messages.append({\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Add comprehensive documentation to this function, including description, parameters, \"\n",
    "                 \"return value, examples, and edge cases. Output the function in a ```python code block```.\"\n",
    "   })\n",
    "   documented_function = generate_response(messages)\n",
    "   documented_function = extract_code_block(documented_function)\n",
    "   print(\"\\n=== Documented Function ===\")\n",
    "   print(documented_function)\n",
    "\n",
    "   # Add documentation response to conversation\n",
    "   messages.append({\"role\": \"assistant\", \"content\": \"\\`\\`\\`python\\n\\n\"+documented_function+\"\\n\\n\\`\\`\\`\"})\n",
    "\n",
    "   # Third prompt - Add test cases\n",
    "   messages.append({\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Add unittest test cases for this function, including tests for basic functionality, \"\n",
    "                 \"edge cases, error cases, and various input scenarios. Output the code in a \\`\\`\\`python code block\\`\\`\\`.\"\n",
    "   })\n",
    "   test_cases = generate_response(messages)\n",
    "   # We will likely run into random problems here depending on if it outputs JUST the test cases or the\n",
    "   # test cases AND the code. This is the type of issue we will learn to work through with agents in the course.\n",
    "   test_cases = extract_code_block(test_cases)\n",
    "   print(\"\\n=== Test Cases ===\")\n",
    "   print(test_cases)\n",
    "\n",
    "   # Generate filename from function description\n",
    "   filename = function_description.lower()\n",
    "   filename = ''.join(c for c in filename if c.isalnum() or c.isspace())\n",
    "   filename = filename.replace(' ', '_')[:30] + '.py'\n",
    "\n",
    "   # Save final version\n",
    "   with open(filename, 'w') as f:\n",
    "      f.write(documented_function + '\\n\\n' + test_cases)\n",
    "\n",
    "   return documented_function, test_cases, filename\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "   function_code, tests, filename = develop_custom_function()\n",
    "   print(f\"\\nFinal code has been saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675e22c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
